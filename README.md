# NYC Taxi Data Pipeline

This project is a modular, production-ready data engineering pipeline that downloads, transforms, and uploads NYC Taxi data. It is designed to mimic a real-world data pipeline suitable for AWS S3 + Athena querying.

----------
## ğŸ“¥ Data Source

The NYC Taxi trip data was sourced from the official New York City Taxi & Limousine Commission (TLC).

These files were converted to Parquet format and hosted in a public GitHub repository as part of this data engineering project.

----------

## ğŸ§© Components

Script

Description

`download_data.py`

Downloads Parquet files from a GitHub folder using the GitHub API and combines them locally.

`transform_data.py`

Cleans and transforms the Parquet files (e.g., datetime handling, nulls).

`upload_to_s3.py`

Uploads transformed files to a specified S3 bucket.

`main_pipeline.py`

Orchestrates the entire workflow (download â†’ transform â†’ upload).

`logger_setup.py`

Configures rotating file-based logging shared by all modules.

----------

## ğŸ› ï¸ Requirements

-   Python 3.11+
    
-   AWS credentials set up locally (via `~/.aws/credentials`, environment variables, or IAM Role)
    
-   Python packages:
    
    ```
    pip install pandas pyarrow boto3 requests
    ```
    

----------

## ğŸ—‚ï¸ Directory Structure

```
nyc_taxi_data_pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source/           # Original Parquet files (from GitHub)
â”‚   â”œâ”€â”€ processed/        # Transformed files ready for S3
â”‚   â””â”€â”€ error/            # Files that failed during transformation
â”œâ”€â”€ logs/                 # Rotating log files
â”œâ”€â”€ download_data.py
â”œâ”€â”€ transform_data.py
â”œâ”€â”€ upload_to_s3.py
â”œâ”€â”€ main_pipeline.py
â”œâ”€â”€ logger_setup.py
â””â”€â”€ README.md
```

----------

## ğŸš€ How to Run the Pipeline

1.  **Ensure your AWS credentials are configured** (see below).
    
2.  **Run the entire pipeline**:
    

```
python main_pipeline.py
```

This will:

-   Download Parquet files from GitHub (or any configured source)
    
-   Clean and transform them
    
-   Upload them to the configured S3 bucket
    

----------

## ğŸ” AWS Credentials Setup

### Option 1: Environment Variables

```
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
export AWS_DEFAULT_REGION=ap-southeast-2
```

### Option 2: AWS CLI Profile (~/.aws/credentials)

```
[default]
aws_access_key_id = YOUR_KEY
aws_secret_access_key = YOUR_SECRET
```

----------

## ğŸ§ª Testing Individual Steps

### Run Download:

```
python download_data.py
```

### Run Transform:

```
python transform_data.py --input-dir data
```

### Run Upload:

```
python upload_to_s3.py --bucket your-bucket-name --input-dir data/processed
```

----------

## ğŸ“‹ Logging

All logs are stored in the `logs/` folder with log rotation (2 backups, 1MB each).

----------

## ğŸ“Œ Notes

-   GitHub API is used to fetch a dynamic list of `.parquet` files.
    
-   PyArrow and pandas are used for efficient Parquet processing.
    
-   AWS S3 key prefix is customizable.
    

----------

## ğŸ§‘ Author

**CK**, 2025-Jul

MIT License
